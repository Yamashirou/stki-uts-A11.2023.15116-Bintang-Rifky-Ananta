{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTS_STKI \u2014 Bintang Rifky Ananta\n\n**Notebook**: preprocessing \u2192 Boolean / VSM / BM25 \u2192 evaluation & visualisasi\n\nDataset: `ulasan_enterkomputer_tokopedia.csv` + `ulasan_enterkomputer_gmaps.csv` (digabung).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & setup\nimport os, sys\nsys.path.insert(0, os.path.abspath('..'))\nprint('Added project root to sys.path:', os.path.abspath('..'))\n\nimport pandas as pd, numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom src.preprocess import preprocess_text\nfrom src.boolean_ir import build_inverted_index\nfrom src import vsm_ir\nfrom src import eval as ir_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV datasets (Tokopedia + GMaps)\ntok_path = '/mnt/data/ulasan_enterkomputer_tokopedia.csv'\ngmaps_path = '/mnt/data/ulasan_enterkomputer_gmaps.csv'\n\ndf_tok = pd.read_csv(tok_path)\ndf_gmaps = pd.read_csv(gmaps_path)\n\nprint('Tokopedia columns:', df_tok.columns.tolist())\nprint('GMaps columns:', df_gmaps.columns.tolist())\n\ndef find_text_column(df):\n    candidates = ['review','ulasan','komentar','text','content','review_text','comment','body','Review Text','reviewText']\n    for c in candidates:\n        if c in df.columns:\n            return c\n    for c in df.columns:\n        if df[c].dtype == object:\n            return c\n    raise ValueError('No text column found')\n\ntxt_col_tok = find_text_column(df_tok)\ntxt_col_gmaps = find_text_column(df_gmaps)\nprint('Using text columns:', txt_col_tok, 'and', txt_col_gmaps)\n\ndf_tok = df_tok.rename(columns={txt_col_tok: 'text'})\ndf_gmaps = df_gmaps.rename(columns={txt_col_gmaps: 'text'})\ndf = pd.concat([df_tok[['text']], df_gmaps[['text']]], ignore_index=True).reset_index(drop=True)\nprint('Combined dataframe shape:', df.shape)\ndf.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 15 documents from combined data (random but reproducible)\ndf = df[df['text'].notna() & df['text'].astype(str).str.len()>10].copy()\ndf_sample = df.sample(n=min(15, len(df)), random_state=42).reset_index(drop=True)\nprint('Sampled', len(df_sample), 'documents.')\nfor i, t in enumerate(df_sample['text'][:3], start=1):\n    print(f'--- Doc {i} ---\\n', t[:300].replace('\\n',' '), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess sampled docs using src.preprocess.preprocess_text\nprocessed_tokens = []\nprocessed_texts = []\nfor txt in df_sample['text']:\n    toks = preprocess_text(txt)\n    processed_tokens.append(toks)\n    processed_texts.append(' '.join(toks))\n\ndf_sample['processed'] = processed_texts\n\n# Show before/after for first 2 docs\nfor i in range(min(2, len(df_sample))):\n    print(f'=== Document {i+1} ORIGINAL ===\\n{df_sample.loc[i,\"text\"][:500]}\\n')\n    print(f'--- Processed tokens ({len(processed_tokens[i])}) ---\\n{processed_tokens[i][:60]}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build inverted index (Boolean retrieval)\ndocs = {f'doc{idx+1:02d}.txt': txt for idx, txt in enumerate(df_sample['processed'])}\ninv_index = build_inverted_index(docs)\nprint('Vocabulary size (inverted index):', len(inv_index))\nfor term in list(inv_index.keys())[:8]:\n    print(term, '->', inv_index[term])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TF-IDF matrix using scikit-learn for convenience\nvectorizer = TfidfVectorizer(use_idf=True, norm='l2')\ntfidf_matrix = vectorizer.fit_transform(df_sample['processed'])  # shape (n_docs, n_terms)\nprint('TF-IDF matrix shape:', tfidf_matrix.shape)\n\ndef search_top_k_vsm(query, k=5):\n    q_toks = preprocess_text(query)\n    q_text = ' '.join(q_toks)\n    q_vec = vectorizer.transform([q_text])\n    sims = cosine_similarity(q_vec, tfidf_matrix)  # shape (1, n_docs)\n    top_idx = sims[0].argsort()[-k:][::-1]\n    return top_idx, sims[0][top_idx]\n\n# test search\nquery = 'aplikasi crypto'\ntop_idx, scores = search_top_k_vsm(query, k=5)\nprint('Top docs (VSM) for query:', query)\nfor idx, s in zip(top_idx, scores):\n    print(idx, f'(score={s:.4f})', df_sample.loc[idx,'text'][:200].replace('\\n',' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 ranking using src.vsm_ir implementation (expects processed tokens joined by space)\nbm25_index = vsm_ir.build_bm25(docs)  # docs is dict created earlier where values are processed joined text\ndef search_top_k_bm25(query, k=5):\n    q_toks = preprocess_text(query)\n    q_text = ' '.join(q_toks)\n    return vsm_ir.score_bm25(q_text, bm25_index, topk=k)\n\n# test BM25\nbm25_results = search_top_k_bm25('garansi rusak', k=5)\nprint('Top docs (BM25):')\nfor doc, sc in bm25_results:\n    idx = int(doc.replace('doc','').replace('.txt',''))-1\n    print(doc, f'(score={sc:.4f})', df_sample.loc[idx,'text'][:200].replace('\\n',' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: create a simple gold set based on keywords\ngold_queries = {\n    'pelayanan baik': [i for i, t in enumerate(df_sample['processed']) if 'pelayanan' in t or 'layanan' in t],\n    'pengiriman cepat': [i for i, t in enumerate(df_sample['processed']) if 'kirim' in t or 'pengiriman' in t],\n    'garansi': [i for i, t in enumerate(df_sample['processed']) if 'garansi' in t]\n}\nprint('Gold set (example):', gold_queries)\n\n# Evaluate for one query using VSM results\nq = 'pelayanan baik'\ntop_idx, scores = search_top_k_vsm(q, k=5)\nretrieved_docs = [f'doc{i+1:02d}.txt' for i in top_idx]\nrelevant_docs = [f'doc{i+1:02d}.txt' for i in gold_queries[q]]\nprint('Retrieved:', retrieved_docs)\nprint('Relevant (gold):', relevant_docs)\n\nprec = ir_eval.precision(retrieved_docs, relevant_docs)\nrec = ir_eval.recall(retrieved_docs, relevant_docs)\nf1 = ir_eval.f1_score(retrieved_docs, relevant_docs)\nprint(f'Precision@5: {prec:.4f}, Recall@5: {rec:.4f}, F1: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VSM top-k results bar chart for the query used earlier\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,4))\nlabels = [f'doc{idx+1:02d}' for idx in top_idx]\nplt.bar(labels, scores)\nplt.xlabel('Document ID')\nplt.ylabel('Cosine similarity')\nplt.title(f'VSM Top-{len(labels)} results for query: \"{query}\"')\nplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sampled docs to data/raw for reproducibility (optional)\nout_dir = os.path.abspath(os.path.join('..','data','raw'))\nos.makedirs(out_dir, exist_ok=True)\nfor i, txt in enumerate(df_sample['text']):\n    fname = os.path.join(out_dir, f'doc{i+1:02d}.txt')\n    with open(fname, 'w', encoding='utf-8') as f:\n        f.write(txt)\nprint('Saved sampled docs to', out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kesimpulan singkat\n\n- Notebook ini menunjukkan pipeline STKI: preprocessing, Boolean retrieval, VSM (TF-IDF) dan BM25.\n- Untuk evaluasi retrieval yang lebih valid, buat gold set manual (relevansi per query) dan jalankan metrik secara terstruktur (Precision@k, MAP@k, nDCG@k) untuk beberapa query.\n- Selanjutnya, lengkapi laporan dengan screenshot output notebook, tabel metrik, dan diskusi perbandingan TF-IDF vs BM25.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}